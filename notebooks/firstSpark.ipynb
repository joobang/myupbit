{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(\"1\",100), (\"2\",200), (\"3\",300)]).toDF(\"id\",\"value\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id\tvalue\n",
    "1\t100\n",
    "2\t200\n",
    "3\t300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"Spark\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(\"1\",100), (\"2\",200), (\"3\",300)]).toDF(\"id\",\"value\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id\tvalue\n",
    "1\t100\n",
    "2\t200\n",
    "3\t300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark =  SparkSession.builder \\\n",
    "         .master(\"spark://spark-master:7077\") \\\n",
    "         .appName(\"Spark\") \\\n",
    "         .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "         .config(\"spark.driver.extraClassPath\", \"/usr/local/spark/jars/postgresql-42.2.16.jar\") \\\n",
    "         .getOrCreate()\n",
    "sql = \"\"\"\n",
    "select * from day_candle\n",
    "\"\"\"\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_analytics:5432/upbit\") \\\n",
    "    .option(\"query\", sql) \\\n",
    "    .option(\"user\", \"airflow\") \\\n",
    "    .option(\"password\", \"airflow\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+---+-------+--------------------+--------------------+------------------+------------------+------------------+------------------+--------------+----------------------+-----------------------+------------------+-----------------+------------+---------------------+\n",
    "| id| market|candle_date_time_utc|candle_date_time_kst|     opening_price|        high_price|         low_price|       trade_price|last_timestamp|candle_acc_trade_price|candle_acc_trade_volume|prev_closing_price|     change_price| change_rate|converted_trade_price|\n",
    "+---+-------+--------------------+--------------------+------------------+------------------+------------------+------------------+--------------+----------------------+-----------------------+------------------+-----------------+------------+---------------------+\n",
    "|  1|KRW-BTC| 2023-07-13 00:00:00| 2023-07-13 09:00:00|39803000.000000000|39907000.000000000|39356000.000000000|39850000.000000000| 1689245969994|  71518124677.97214...|         1805.622751130|39804000.000000000|  46000.000000000| 0.001155663|                 null|\n",
    "|  2|KRW-BTC| 2023-07-12 00:00:00| 2023-07-12 09:00:00|40117000.000000000|40600000.000000000|39751000.000000000|39804000.000000000| 1689206399774|  130054585570.2244...|         3241.640081040|40117000.000000000|-313000.000000000|-0.007802179|                 null|\n",
    "|  3|KRW-BTC| 2023-07-11 00:00:00| 2023-07-11 09:00:00|40060000.000000000|40387000.000000000|39946000.000000000|40117000.000000000| 1689119996687|  101896200770.3168...|         2541.057293690|40050000.000000000|  67000.000000000| 0.001672909|                 null|\n",
    "|  4|KRW-BTC| 2023-07-10 00:00:00| 2023-07-10 09:00:00|39839000.000000000|40784000.000000000|39666000.000000000|40050000.000000000| 1689033600015|  113228698197.8694...|         2829.622909540|39832000.000000000| 218000.000000000| 0.005472987|                 null|\n",
    "|  5|KRW-BTC| 2023-07-09 00:00:00| 2023-07-09 09:00:00|40154000.000000000|40302000.000000000|39770000.000000000|39832000.000000000| 1688947199514|  59452578236.03948...|         1485.270126220|40153000.000000000|-321000.000000000|-0.007994421|                 null|\n",
    "|  6|KRW-BTC| 2023-07-08 00:00:00| 2023-07-08 09:00:00|40241000.000000000|40253000.000000000|39920000.000000000|40153000.000000000| 1688860800001|  52108582785.83458...|         1299.849637360|40240000.000000000| -87000.000000000|-0.002162028|                 null|\n",
    "|  7|KRW-BTC| 2023-07-07 00:00:00| 2023-07-07 09:00:00|39864000.000000000|40330000.000000000|39676000.000000000|40240000.000000000| 1688774398138|  107607256181.2230...|         2687.615798380|39860000.000000000| 380000.000000000| 0.009533367|                 null|\n",
    "|  8|KRW-BTC| 2023-07-06 00:00:00| 2023-07-06 09:00:00|40385000.000000000|41500000.000000000|39838000.000000000|39860000.000000000| 1688687999577|  233754843596.7873...|         5759.591300000|40385000.000000000|-525000.000000000|-0.012999876|                 null|\n",
    "|  9|KRW-BTC| 2023-07-05 00:00:00| 2023-07-05 09:00:00|40610000.000000000|40775000.000000000|39801000.000000000|40385000.000000000| 1688601597893|  144846374468.0306...|         3595.751885990|40610000.000000000|-225000.000000000|-0.005540507|                 null|\n",
    "| 10|KRW-BTC| 2023-07-04 00:00:00| 2023-07-04 09:00:00|41200000.000000000|41388000.000000000|40540000.000000000|40610000.000000000| 1688515199499|  132297295113.1266...|         3230.332133850|41200000.000000000|-590000.000000000|-0.014320388|                 null|\n",
    "| 11|KRW-BTC| 2023-07-03 00:00:00| 2023-07-03 09:00:00|40652000.000000000|41377000.000000000|40470000.000000000|41200000.000000000| 1688428798697|  133826565121.0243...|         3276.857912610|40652000.000000000| 548000.000000000| 0.013480272|                 null|\n",
    "| 12|KRW-BTC| 2023-07-02 00:00:00| 2023-07-02 09:00:00|40712000.000000000|40875000.000000000|40250000.000000000|40652000.000000000| 1688342399795|  90738583549.27074...|         2235.268698480|40691000.000000000| -39000.000000000|-0.000958443|                 null|\n",
    "| 13|KRW-BTC| 2023-07-01 00:00:00| 2023-07-01 09:00:00|40547000.000000000|40877000.000000000|40391000.000000000|40691000.000000000| 1688255997808|  110148710237.6678...|         2709.937087650|40547000.000000000| 144000.000000000| 0.003551434|                 null|\n",
    "| 14|KRW-BTC| 2023-06-30 00:00:00| 2023-06-30 09:00:00|40583000.000000000|41569000.000000000|39500000.000000000|40547000.000000000| 1688169599322|  348868924510.8400...|         8585.543862050|40583000.000000000| -36000.000000000|-0.000887071|                 null|\n",
    "| 15|KRW-BTC| 2023-06-29 00:00:00| 2023-06-29 09:00:00|40017000.000000000|41048000.000000000|39997000.000000000|40583000.000000000| 1688083199722|  141777245742.6427...|         3490.488264620|40020000.000000000| 563000.000000000| 0.014067966|                 null|\n",
    "| 16|KRW-BTC| 2023-06-28 00:00:00| 2023-06-28 09:00:00|40587000.000000000|40629000.000000000|39966000.000000000|40020000.000000000| 1687996796555|  120211163983.3608...|         2988.757823730|40585000.000000000|-565000.000000000|-0.013921400|                 null|\n",
    "| 17|KRW-BTC| 2023-06-27 00:00:00| 2023-06-27 09:00:00|40094000.000000000|40950000.000000000|39963000.000000000|40585000.000000000| 1687910399826|  166867348632.3628...|         4130.902658710|40077000.000000000| 508000.000000000| 0.012675600|                 null|\n",
    "| 18|KRW-BTC| 2023-06-26 00:00:00| 2023-06-26 09:00:00|40352000.000000000|40599000.000000000|39555000.000000000|40077000.000000000| 1687823997500|  198612849741.0181...|         4959.925029990|40352000.000000000|-275000.000000000|-0.006815028|                 null|\n",
    "| 19|KRW-BTC| 2023-06-25 00:00:00| 2023-06-25 09:00:00|40659000.000000000|41399000.000000000|40197000.000000000|40352000.000000000| 1687737599443|  182726918565.7282...|         4478.466285950|40659000.000000000|-307000.000000000|-0.007550604|                 null|\n",
    "| 20|KRW-BTC| 2023-06-24 00:00:00| 2023-06-24 09:00:00|40651000.000000000|40931000.000000000|40383000.000000000|40659000.000000000| 1687651199607|  160034811596.6366...|         3933.825602180|40651000.000000000|   8000.000000000| 0.000196797|                 null|\n",
    "+---+-------+--------------------+--------------------+------------------+------------------+------------------+------------------+--------------+----------------------+-----------------------+------------------+-----------------+------------+---------------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "if sc is not None:\n",
    "    print(\"There is an active Spark Context.\")\n",
    "    sc.stop()  # This is how you stop a SparkContext\n",
    "else:\n",
    "    print(\"There is no active Spark Context.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": java.lang.IllegalStateException: LiveListenerBus is stopped.\n",
    "주피터 노트북에서 Spark 작업을 실행할 때에도 동일한 SparkContext를 여러 번 재사용하거나 종료 후 다시 시작하는 경우 위와 같은 오류가 발생할 수 있습니다. 일반적으로, 한 주피터 노트북에서는 하나의 SparkSession만 실행해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "price_df = jdbcDF.select('candle_date_time_utc', 'trade_price')\n",
    "\n",
    "# Pandas DataFrame으로 변환합니다.\n",
    "pd_df = price_df.toPandas()\n",
    "\n",
    "# 날짜를 datetime 형식으로 변환합니다.\n",
    "pd_df['date'] = pd.to_datetime(pd_df['candle_date_time_utc'])\n",
    "\n",
    "# 그래프를 그립니다.\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pd_df['candle_date_time_utc'], pd_df['trade_price'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.title('Bitcoin Price Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드는 날짜 기준으로 종가 그래프를 그리는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "functions does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 최고 가격과 최저 가격의 차이를 계산합니다.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m jdbcDF \u001b[38;5;241m=\u001b[39m jdbcDF\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice_difference\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhigh_price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow_price\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 결과를 확인합니다.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m jdbcDF\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandle_date_time_utc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice_difference\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:160\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:221\u001b[0m, in \u001b[0;36mcol\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;129m@try_remote_functions\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcol\u001b[39m(col: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    Returns a :class:`~pyspark.sql.Column` based on the given column name.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    Column<'x'>\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:94\u001b[0m, in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03mInvokes JVM function identified by name with args\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m jf \u001b[38;5;241m=\u001b[39m \u001b[43m_get_jvm_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_active_spark_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jf(\u001b[38;5;241m*\u001b[39margs))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:85\u001b[0m, in \u001b[0;36m_get_jvm_function\u001b[0;34m(name, sc)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mRetrieves JVM function identified by name from\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mJava gateway associated with sc.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m, name)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: functions does not exist in the JVM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# 최고 가격과 최저 가격의 차이를 계산합니다.\n",
    "jdbcDF = jdbcDF.withColumn('price_difference', col('high_price') - col('low_price'))\n",
    "\n",
    "# 결과를 확인합니다.\n",
    "jdbcDF.select('candle_date_time_utc', 'price_difference').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일일 최고 가격과 최저 가격의 차이 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "# 거래량과 가격 간의 상관관계를 계산합니다.\n",
    "correlation = jdbcDF.stat.corr('candle_acc_trade_volume', 'trade_price')\n",
    "\n",
    "print(\"Correlation between volume and price: \", correlation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상관계수(correlation coefficient)는 두 변수 사이의 선형적 관계를 나타낸다.\n",
    "값의 범위는 -1부터 1까지이고 1에 가까울수록 양의 선형관계를 나타낸다.\n",
    "이것은 한 변수의 값이 증가할 때 다른 변수의 값도 증가하는 경향을 보임을 의미한다.\n",
    "-1에 가까울수록 음의 선형관계를 나타내고 이는 한 변수의 값이 증가할때 다른 변수의 값이 감소하는 경향을 보임을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Define the window\n",
    "windowSpec = Window.orderBy('candle_date_time_utc').rowsBetween(-6, 0)\n",
    "\n",
    "# Calculate the moving average\n",
    "df = jdbcDF.withColumn('rolling_avg_high_price', avg(jdbcDF['high_price']).over(windowSpec))\n",
    "\n",
    "df.select('candle_date_time_utc', 'opening_price','low_price','price_difference','rolling_avg_high_price').show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rolling_avg_high_price는 일정 기간 동안의 'high_price' (최고 가격)의 이동 평균을 나타낸다. 이동 평균은 통계에서 자주 사용되는 기법으로, 일정 기간의 평균을 계산하여 시계열 데이터의 잡음을 줄이고 전반적인 경향성을 확인하는 데 도움이 된다.\n",
    "\n",
    "이 경우, rolling_avg_high_price는 최근 7일간의 최고 가격의 평균을 나타내고 이 값을 통해 최근 일주일 동안 비트코인의 최고 가격이 어떻게 변화하고 있는지를 파악할 수 있다. 이 값이 시간이 지남에 따라 증가하면 가격이 상승하고 있는 것이고, 감소하면 가격이 하락하고 있는 것으로 해석할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
